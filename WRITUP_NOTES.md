
# OCR A Level Computer Science (H446) — Programming Project (NEA) Mark Scheme (Summary)

> Paraphrased summary of the official marking criteria for **Component 03/04: Programming Project**.  
> Total: **70 marks** across Analysis, Design, Development, and Evaluation.

> **Note:** This is a student-friendly paraphrase to help you plan and self-assess. For formal wording, always consult the official OCR specification and materials.

---

## Overview of Marks

- **Analysis (AO2.2)** — **10 marks**
- **Design** — **15 marks**
- **Development** — **25 marks**  
  - *Technical solution* — **15**  
  - *Testing to inform development* — **10**
- **Evaluation** — **20 marks**  
  - *Evaluation write-up* — **15**  
  - *Testing to inform evaluation* — **5**

**Total:** 10 + 15 + (15 + 10) + (15 + 5) = **70**.

---

## What the examiners look for (in plain English)

### 1) Analysis (10)
- Identify a **real, bounded problem** with real users/stakeholders and constraints.
- Justify **why it suits a computational solution**; define measurable **success criteria**.
- Gather and summarise **evidence**: user interviews/surveys, domain research, data sources, legal/ethical considerations.
- Show a clear **problem decomposition** (what the system must do and why).

**Evidence to include**
- Problem statement, stakeholders, goals, constraints.
- User stories or use cases; measurable success criteria.
- Research notes and sources.

---

### 2) Design (15)
- Propose a **solution architecture** (modules/components) and **data design** (structures, schemas, persistence).
- Specify **algorithms** (pseudocode/flowcharts), validation, error handling, security/usability considerations.
- Include **UI/UX sketches** or wireframes if there is a user interface.
- Plan **testing** for development (what, when, and why).

**Evidence to include**
- High-level design with diagrams (e.g., component/class, data models).
- Algorithm descriptions (pseudocode) aligned to success criteria.
- Test plan for the development phase.

---

### 3) Development (25)
#### 3a) Technical solution (15)
- Implement a **working, maintainable** program using appropriate **abstractions** (functions/classes/modules).
- Demonstrate **sensible algorithms & data structures**, plus **defensive programming** where appropriate.
- Show **version control** and **iterative progress**; justify key implementation choices.
- Code should be readable (naming, comments/docstrings) and organised.

#### 3b) Testing to inform development (10)
- Use **iterative testing** while building: unit/integration/feature tests, plus ad‑hoc where useful.
- Record **test cases, data, outcomes, fixes**; show how tests **changed your next steps**.
- Include **usability checks** (where relevant) and evidence of **bug-fixing**.

**Evidence to include**
- Source code (well structured), commit history, build/run instructions.
- Development test logs/tables with before/after screenshots and commentary.
- Rationale for libraries/tech choices.

---

### 4) Evaluation (20)
#### 4a) Evaluation write‑up (15)
- Measure the final product **against success criteria** with real data/tasks.
- Reflect on **strengths, limitations, trade‑offs**, and how the design met user needs.
- Discuss **performance/complexity**, maintainability, and **further improvements**.
- Consider **ethical/legal** aspects (e.g., data protection) where relevant.

#### 4b) Testing to inform evaluation (5)
- Provide a **final round of testing** (including user acceptance where possible) designed to evaluate the whole system.
- Show **representative test data**, outcomes, and interpretation.

**Evidence to include**
- Evaluation report mapped to each success criterion (pass/partial/fail + evidence).
- Final test summary, user feedback, and next-steps plan.

---

## Banding guidance (how to aim higher)

- **Top bands**: Clear reasoning throughout, strong linkage between analysis → design → development → evaluation; sophisticated algorithms/data structures; thorough, purposeful testing that drives decisions; critical, evidence‑backed evaluation.
- **Mid bands**: Mostly complete with some gaps; testing documented but not consistently driving changes; design and analysis present but could be tighter.
- **Lower bands**: Vague problem/criteria, limited design detail, ad‑hoc development with minimal testing, evaluation descriptive rather than evidence‑based.

---

## Practical submission checklist

- [ ] **Problem & users** clearly defined; success criteria measurable.
- [ ] **Design pack**: architecture, data design, algorithms, UI/UX (if relevant), dev test plan.
- [ ] **Working program** with readable, modular code and instructions to run.
- [ ] **Iterative testing log** that shows bugs found, fixes, and impact on next steps.
- [ ] **Evaluation** mapped to criteria with final testing/feedback.
- [ ] **Appendices**: raw test data, screenshots, survey instruments, consent (if needed).

---

## Sources (for official wording)
- OCR **H446 Specification** — Programming project (Component 03/04) marking criteria (70 marks).  
- OCR **NEA Cover Sheet** for H446/03–04 (lists the sub‑components used in marking).

*(This file is a paraphrase/summary to help you plan; always refer to OCR’s official PDFs when marking or submitting.)*


--- 
--- 
--- 

## Analysis - 10 marks

* Described and justified the features that make the problem solvable by computational methods, explaining why it is amenable to a computational approach.

* Identified suitable stakeholders for the project and described them explaining how they will make use of the proposed solution and why it is appropriate to their needs.

* Researched the problem in depth looking at existing solutions to similar problems, identifying and justifying suitable approaches based on this research.

* Identified the essential features of the proposed computational solution explaining these choices.

* Identified and explained with justification any limitations of the proposed solution.

* Specified and justified the requirements for the solution including (as appropriate) any hardware and software requirements.

* Identified and justified measurable success criteria for the proposed

  
